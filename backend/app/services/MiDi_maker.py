"""
ë“œëŸ¼ WAV íŒŒì¼ â†’ MIDI ì•…ë³´ ë³€í™˜ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ê°œì„  ë²„ì „)
CNN+BiGRU ëª¨ë¸ì„ ì‚¬ìš©í•œ ë“œëŸ¼ íƒ€ê²© ê²€ì¶œ ë° MIDI ìƒì„±

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. âœ… Mel normalization ì ìš© (BiGRU_datautilr_final.pyì™€ ë™ì¼)
2. âœ… Checkpointì—ì„œ ìµœì  threshold ìë™ ë¡œë“œ
3. âœ… ë“œëŸ¼ë³„ ë…ë¦½ì ì¸ ì–‘ìí™” ê·¸ë¦¬ë“œ ì„¤ì •
4. âœ… Rising edge detectionìœ¼ë¡œ onset delay ìµœì†Œí™”
5. âœ… Floor-based quantizationìœ¼ë¡œ íƒ€ì´ë° ì‹œí”„íŠ¸ ë°©ì§€

ì‚¬ìš©ë²•:
    ì½”ë“œ ë§¨ ì•„ë˜ ì‹¤í–‰ ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ìˆ˜ì • í›„ ì‹¤í–‰
    python MiDi_maker_final.py
"""

import os
import math
import numpy as np
import torch
import librosa
import pretty_midi
from pathlib import Path
from typing import List, Tuple, Dict
import warnings

warnings.filterwarnings('ignore')

from app.services.BiGRU_model import DrumOnsetDetector


# ============================================
# ì„¤ì • í´ë˜ìŠ¤
# ============================================
class InferenceConfig:
    """ì¶”ë¡  ë° MIDI ë³€í™˜ ì„¤ì • (ìµœì¢… ê°œì„  ë²„ì „)"""

    def __init__(self):
        # ============================================
        # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì„¤ì • (BiGRU_datautilr_final.pyì™€ ë™ì¼!)
        # ============================================
        self.sample_rate = 22050
        self.n_fft = 2048
        self.hop_length = 256
        self.n_mels = 128
        self.fmin = 20
        self.fmax = 8000

        # Mel normalization ì ìš© (ê°œì„ !)
        self.use_mel_normalization = True

        # ì‹œê°„ í•´ìƒë„
        self.frame_duration = self.hop_length / self.sample_rate  # ~11.6ms

        # ============================================
        # ëª¨ë¸ ì„¤ì • (BiGRU_model.pyì™€ ë™ì¼!)
        # ============================================
        self.n_classes = 3  # kick, snare, hihat
        self.cnn_channels = [32, 64, 128]
        self.gru_hidden = 384
        self.gru_layers = 2
        self.dropout = 0.3

        # ============================================
        # Sliding Window ì„¤ì •
        # ============================================
        self.window_size = 2000  # í”„ë ˆì„ (~23ì´ˆ)
        self.hop_size = 1000  # 50% overlap

        # ============================================
        # íƒ€ê²© ê²€ì¶œ ì„ê³„ê°’ (checkpointì—ì„œ ìë™ ë¡œë“œë¨)
        # ============================================
        self.thresholds = {
            'kick': 0.45,
            'snare': 0.45,
            'hihat': 0.35  # ê¸°ë³¸ê°’
        }

        # ============================================
        # BPM ìë™ ê°ì§€ ì„¤ì •
        # ============================================
        self.bpm_start_range = 60
        self.bpm_end_range = 200

        # ============================================
        # ì–‘ìí™” ì„¤ì • (ë“œëŸ¼ë³„ ë…ë¦½ ê·¸ë¦¬ë“œ)
        # ============================================
        self.grid_division = {
            'kick': 16,  # 16ë¶„ìŒí‘œ ê·¸ë¦¬ë“œ
            'snare': 16,
            'hihat': 8   # 8ë¶„ìŒí‘œ ê·¸ë¦¬ë“œ
        }

        self.default_grid_division = 16

        # ============================================
        # í›„ì²˜ë¦¬ ì„¤ì • (4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸)
        # ============================================
        # Stage 1: ì¸ì ‘ ì´ë²¤íŠ¸ ë³‘í•©
        self.merge_window_ms = 50

        # Stage 2: ìµœì†Œ ê°„ê²© ê°•ì œ
        self.min_gap_ms = {
            'kick': 80,
            'snare': 60,
            'hihat': 30
        }

        # Stage 3: ê·¸ë¦¬ë“œ ì–‘ìí™” ë°”ì´ì–´ìŠ¤
        self.quantize_bias = {
            'kick': 0.3,
            'snare': 0.3,
            'hihat': 0.25
        }

        # Stage 4: ë™ì‹œ íƒ€ê²© í—ˆìš© ë²”ìœ„
        self.simultaneous_window_ms = 30

        # ============================================
        # MIDI ì¶œë ¥ ì„¤ì • (General MIDI Drum Map)
        # ============================================
        self.midi_mapping = {
            'kick': 36,  # Bass Drum 1
            'snare': 38,  # Acoustic Snare
            'hihat': 42  # Closed Hi-Hat
        }

        self.velocity = 100
        self.note_duration = 0.1

        # ============================================
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        # ============================================
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        if self.device.type == 'cuda':
            try:
                test_tensor = torch.zeros(1).to(self.device)
                del test_tensor
                print(f"âœ… GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
            except:
                print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€, CPUë¡œ ì „í™˜")
                self.device = torch.device('cpu')
        else:
            print("â„¹ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰")

    def get_grid_interval(self, bpm: float, drum_type: str = None) -> float:
        """BPMê³¼ ë“œëŸ¼ íƒ€ì…ì— ë”°ë¥¸ ê·¸ë¦¬ë“œ ê°„ê²© ê³„ì‚°"""
        beat_duration = 60.0 / bpm

        if drum_type and drum_type in self.grid_division:
            division = self.grid_division[drum_type]
        else:
            division = self.default_grid_division

        grid_interval = beat_duration / (division / 4)
        return grid_interval

    def get_quantize_bias(self, drum_type: str) -> float:
        """ë“œëŸ¼ë³„ ì–‘ìí™” ë°”ì´ì–´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        return self.quantize_bias.get(drum_type, 0.3)

    def load_thresholds_from_checkpoint(self, checkpoint: dict):
        """
        Checkpointì—ì„œ ìµœì  threshold ë¡œë“œ (ê°œì„ !)

        Args:
            checkpoint: í•™ìŠµëœ ëª¨ë¸ checkpoint
        """
        if 'config' in checkpoint and 'thresholds' in checkpoint['config']:
            thresholds_list = checkpoint['config']['thresholds']

            # List to dict conversion
            drum_types = ['kick', 'snare', 'hihat']
            for i, drum_type in enumerate(drum_types):
                if i < len(thresholds_list):
                    self.thresholds[drum_type] = thresholds_list[i]

            print(f"\nâœ… Checkpointì—ì„œ ìµœì  threshold ë¡œë“œ:")
            print(f"   Kick:  {self.thresholds['kick']:.2f}")
            print(f"   Snare: {self.thresholds['snare']:.2f}")
            print(f"   Hihat: {self.thresholds['hihat']:.2f}")
        else:
            print(f"\nâš ï¸  Checkpointì— threshold ì •ë³´ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")


# ============================================
# Mel Normalization (ê°œì„ !)
# ============================================
def normalize_mel(mel_spec: np.ndarray) -> np.ndarray:
    """
    ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì •ê·œí™” (BiGRU_datautilr_final.pyì™€ ë™ì¼)

    ê°œì„ : ë…¹ìŒ ì„¸ê¸°ì— ë”°ë¥¸ dynamic range ë³€ë™ ì™„í™”
    """
    mean = np.mean(mel_spec)
    std = np.std(mel_spec)

    if std > 1e-5:
        mel_spec = (mel_spec - mean) / std

    return mel_spec


# ============================================
# WAV â†’ Mel Spectrogram (ê°œì„ !)
# ============================================
def load_and_preprocess_audio(wav_path: str, config: InferenceConfig) -> Tuple[np.ndarray, float]:
    """
    WAV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜

    ê°œì„ : Mel normalization ì¶”ê°€
    """
    print(f"\nğŸ“‚ ì˜¤ë””ì˜¤ ë¡œë”©: {wav_path}")

    # WAV ë¡œë“œ
    y, sr = librosa.load(wav_path, sr=config.sample_rate, mono=True)
    duration = len(y) / sr

    print(f"   ìƒ˜í”Œë ˆì´íŠ¸: {sr} Hz")
    print(f"   ê¸¸ì´: {duration:.2f}ì´ˆ")
    print(f"   ìƒ˜í”Œ ìˆ˜: {len(y):,}")

    # ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë³€í™˜
    mel_spec = librosa.feature.melspectrogram(
        y=y,
        sr=sr,
        n_fft=config.n_fft,
        hop_length=config.hop_length,
        n_mels=config.n_mels,
        fmin=config.fmin,
        fmax=config.fmax
    )

    # dB ìŠ¤ì¼€ì¼ ë³€í™˜
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_spec_db = mel_spec_db.T  # (n_frames, n_mels)

    # Mel normalization ì ìš© (ê°œì„ !)
    if config.use_mel_normalization:
        mel_spec_db = normalize_mel(mel_spec_db)
        print(f"   âœ… Mel normalization ì ìš©")

    print(f"   ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨ shape: {mel_spec_db.shape}")
    print(f"   í”„ë ˆì„ ìˆ˜: {mel_spec_db.shape[0]}")
    print(f"   í”„ë ˆì„ ê°„ê²©: {config.frame_duration * 1000:.1f}ms (hop_length={config.hop_length})")

    return mel_spec_db, duration


# ============================================
# BPM ìë™ ê°ì§€
# ============================================
def detect_bpm(wav_path: str, config: InferenceConfig) -> float:
    """BPM ìë™ ê°ì§€ (librosa beat tracking)"""
    print(f"\nğŸµ BPM ìë™ ê°ì§€ ì¤‘...")

    y, sr = librosa.load(wav_path, sr=config.sample_rate, mono=True)

    tempo, _ = librosa.beat.beat_track(
        y=y,
        sr=sr,
        start_bpm=120.0
    )

    bpm = float(tempo)
    if bpm < config.bpm_start_range:
        bpm = bpm * 2
    elif bpm > config.bpm_end_range:
        bpm = bpm / 2

    bpm = np.clip(bpm, config.bpm_start_range, config.bpm_end_range)

    print(f"   ê°ì§€ëœ BPM: {bpm:.1f}")
    return bpm


# ============================================
# ëª¨ë¸ ì¶”ë¡  (Sliding Window)
# ============================================
def predict_drum_onsets(
    mel_spec: np.ndarray,
    model: torch.nn.Module,
    config: InferenceConfig
) -> np.ndarray:
    """Sliding window ë°©ì‹ìœ¼ë¡œ ë“œëŸ¼ íƒ€ê²© ì˜ˆì¸¡"""
    print(f"\nğŸ”® ë“œëŸ¼ íƒ€ê²© ì˜ˆì¸¡ ì¤‘...")

    model.eval()
    n_frames = mel_spec.shape[0]
    predictions = np.zeros((n_frames, config.n_classes), dtype=np.float32)
    count_map = np.zeros(n_frames, dtype=np.float32)

    print(f"   ì´ í”„ë ˆì„: {n_frames}")
    print(f"   ìœˆë„ìš° í¬ê¸°: {config.window_size}")
    print(f"   í™‰ í¬ê¸°: {config.hop_size}")

    with torch.no_grad():
        start = 0
        window_idx = 0

        while start < n_frames:
            end = min(start + config.window_size, n_frames)

            window = mel_spec[start:end]
            window_tensor = torch.FloatTensor(window).unsqueeze(0).to(config.device)

            logits = model(window_tensor)
            probs = torch.sigmoid(logits).cpu().numpy()[0]

            actual_len = end - start
            predictions[start:end] += probs[:actual_len]
            count_map[start:end] += 1

            window_idx += 1
            start += config.hop_size

    count_map[count_map == 0] = 1
    predictions = predictions / count_map[:, np.newaxis]

    print(f"   âœ… ì˜ˆì¸¡ ì™„ë£Œ (ìœˆë„ìš° ìˆ˜: {window_idx})")
    return predictions


# ============================================
# Peak Detection (Rising Edge Detection)
# ============================================
def detect_peaks(
        predictions: np.ndarray,
        config: InferenceConfig,
        frame_times: np.ndarray
) -> Dict[str, List[float]]:
    """ì˜ˆì¸¡ í™•ë¥ ì—ì„œ í”¼í¬ë¥¼ ê²€ì¶œí•˜ì—¬ íƒ€ê²© ì‹œê°„ ì¶”ì¶œ"""
    print(f"\nğŸ¯ ë“œëŸ¼ íƒ€ê²© ê²€ì¶œ ì¤‘ (Rising Edge Detection)...")

    drum_types = ['kick', 'snare', 'hihat']
    onsets = {dt: [] for dt in drum_types}

    for drum_idx, drum_type in enumerate(drum_types):
        probs = predictions[:, drum_idx]
        threshold = config.thresholds[drum_type]

        # Rising edge detection
        onset_indices = np.where(
            (probs[:-1] < threshold) & (probs[1:] >= threshold)
        )[0] + 1

        onset_times = [frame_times[idx] for idx in onset_indices]
        onsets[drum_type] = onset_times

        print(f"   {drum_type:6s}: {len(onset_times):4d}ê°œ ê²€ì¶œ (threshold={threshold:.2f})")

    return onsets


# ============================================
# í›„ì²˜ë¦¬ Stage 1: ì¸ì ‘ ì´ë²¤íŠ¸ ë³‘í•©
# ============================================
def merge_nearby_events(
        onsets: Dict[str, List[float]],
        config: InferenceConfig
) -> Dict[str, List[float]]:
    """ì¸ì ‘í•œ ì´ë²¤íŠ¸ë¥¼ ë³‘í•©í•˜ì—¬ ì¤‘ë³µ ì œê±°"""
    print(f"\nğŸ”§ í›„ì²˜ë¦¬ Stage 1: ì¸ì ‘ ì´ë²¤íŠ¸ ë³‘í•© (Â±{config.merge_window_ms}ms)")

    merge_window = config.merge_window_ms / 1000.0
    merged_onsets = {}

    for drum_type, times in onsets.items():
        if len(times) == 0:
            merged_onsets[drum_type] = []
            continue

        times = sorted(times)
        merged = [times[0]]

        for t in times[1:]:
            if t - merged[-1] <= merge_window:
                merged[-1] = merged[-1] * 0.7 + t * 0.3
            else:
                merged.append(t)

        before_count = len(times)
        after_count = len(merged)
        merged_onsets[drum_type] = merged

        print(f"   {drum_type:6s}: {before_count:4d} â†’ {after_count:4d} ({before_count - after_count:3d}ê°œ ë³‘í•©)")

    return merged_onsets


# ============================================
# í›„ì²˜ë¦¬ Stage 2: ìµœì†Œ ê°„ê²© ê°•ì œ
# ============================================
def enforce_minimum_gap(
        onsets: Dict[str, List[float]],
        config: InferenceConfig
) -> Dict[str, List[float]]:
    """ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ ë¹ ë¥¸ ì—°íƒ€ ì œê±°"""
    print(f"\nğŸ”§ í›„ì²˜ë¦¬ Stage 2: ìµœì†Œ ê°„ê²© ê°•ì œ")

    filtered_onsets = {}

    for drum_type, times in onsets.items():
        if len(times) == 0:
            filtered_onsets[drum_type] = []
            continue

        min_gap = config.min_gap_ms[drum_type] / 1000.0
        times = sorted(times)
        filtered = [times[0]]

        for t in times[1:]:
            if t - filtered[-1] >= min_gap:
                filtered.append(t)

        before_count = len(times)
        after_count = len(filtered)
        filtered_onsets[drum_type] = filtered

        print(f"   {drum_type:6s}: {before_count:4d} â†’ {after_count:4d} "
              f"(ìµœì†Œ ê°„ê²© {config.min_gap_ms[drum_type]}ms, {before_count - after_count:3d}ê°œ ì œê±°)")

    return filtered_onsets


# ============================================
# í›„ì²˜ë¦¬ Stage 3: ë“œëŸ¼ë³„ ê·¸ë¦¬ë“œ ê¸°ë°˜ ì–‘ìí™”
# ============================================
def quantize_to_grid(
        onsets: Dict[str, List[float]],
        bpm: float,
        config: InferenceConfig
) -> Dict[str, List[float]]:
    """
    ë“œëŸ¼ë³„ë¡œ ë‹¤ë¥¸ ê·¸ë¦¬ë“œì— ë§ì¶° íƒ€ê²© ì‹œê°„ ì–‘ìí™”
    """
    print(f"\nğŸ”§ í›„ì²˜ë¦¬ Stage 3: ë“œëŸ¼ë³„ ê·¸ë¦¬ë“œ ê¸°ë°˜ ì–‘ìí™” (BPM={bpm:.1f})")

    quantized_onsets = {}

    for drum_type, times in onsets.items():
        if len(times) == 0:
            quantized_onsets[drum_type] = []
            continue

        grid_interval = config.get_grid_interval(bpm, drum_type)
        grid_division = config.grid_division.get(drum_type, config.default_grid_division)
        bias = config.get_quantize_bias(drum_type)

        print(f"   {drum_type:6s}: {grid_division}ë¶„ìŒí‘œ ê·¸ë¦¬ë“œ (ê°„ê²© {grid_interval * 1000:.1f}ms, bias={bias})")

        quantized = []
        for t in times:
            grid_index = math.floor((t / grid_interval) + bias)
            quantized_time = grid_index * grid_interval
            quantized.append(quantized_time)

        quantized = sorted(list(set(quantized)))

        before_count = len(times)
        after_count = len(quantized)
        quantized_onsets[drum_type] = quantized

        print(f"            {before_count:4d} â†’ {after_count:4d} ({before_count - after_count:3d}ê°œ ì¤‘ë³µ ì œê±°)")

    return quantized_onsets


# ============================================
# í›„ì²˜ë¦¬ Stage 4: ë™ì‹œ íƒ€ê²© ê·¸ë£¹í•‘
# ============================================
def group_simultaneous_hits(
        onsets: Dict[str, List[float]],
        config: InferenceConfig
) -> List[Tuple[float, List[str]]]:
    """ë™ì‹œì— ë°œìƒí•˜ëŠ” íƒ€ê²©ì„ ê·¸ë£¹í•‘"""
    print(f"\nğŸ”§ í›„ì²˜ë¦¬ Stage 4: ë™ì‹œ íƒ€ê²© ê·¸ë£¹í•‘ (Â±{config.simultaneous_window_ms}ms)")

    all_events = []
    for drum_type, times in onsets.items():
        for t in times:
            all_events.append((t, drum_type))

    all_events.sort(key=lambda x: x[0])

    if len(all_events) == 0:
        return []

    window = config.simultaneous_window_ms / 1000.0
    grouped_events = []
    current_time = all_events[0][0]
    current_drums = [all_events[0][1]]

    for time, drum in all_events[1:]:
        if time - current_time <= window:
            if drum not in current_drums:
                current_drums.append(drum)
        else:
            grouped_events.append((current_time, sorted(current_drums)))
            current_time = time
            current_drums = [drum]

    grouped_events.append((current_time, sorted(current_drums)))

    multi_hits = sum(1 for _, drums in grouped_events if len(drums) > 1)
    print(f"   ì´ ì´ë²¤íŠ¸: {len(grouped_events)}ê°œ")
    print(f"   ë™ì‹œ íƒ€ê²©: {multi_hits}ê°œ")

    return grouped_events


# ============================================
# MIDI íŒŒì¼ ìƒì„±
# ============================================
def create_midi_file(
        grouped_events: List[Tuple[float, List[str]]],
        bpm: float,
        config: InferenceConfig,
        output_path: str
):
    """ê·¸ë£¹í™”ëœ ì´ë²¤íŠ¸ë¡œë¶€í„° MIDI íŒŒì¼ ìƒì„±"""
    print(f"\nğŸ¼ MIDI íŒŒì¼ ìƒì„± ì¤‘...")

    pm = pretty_midi.PrettyMIDI(initial_tempo=bpm)

    drum_program = 0
    drum_instrument = pretty_midi.Instrument(program=drum_program, is_drum=True, name='Drums')

    for time, drum_types in grouped_events:
        for drum_type in drum_types:
            note_number = config.midi_mapping[drum_type]

            note = pretty_midi.Note(
                velocity=config.velocity,
                pitch=note_number,
                start=time,
                end=time + config.note_duration
            )
            drum_instrument.notes.append(note)

    pm.instruments.append(drum_instrument)
    pm.write(output_path)

    print(f"   âœ… MIDI íŒŒì¼ ì €ì¥: {output_path}")
    print(f"   BPM: {bpm:.1f}")
    print(f"   ì´ ë…¸íŠ¸ ìˆ˜: {len(drum_instrument.notes)}")


# ============================================
# ë””ë²„ê·¸ í…ìŠ¤íŠ¸ ë¡œê·¸ ìƒì„±
# ============================================
def create_debug_log(
        grouped_events: List[Tuple[float, List[str]]],
        bpm: float,
        config: InferenceConfig,
        output_path: str
):
    """ë””ë²„ê·¸ìš© í…ìŠ¤íŠ¸ ë¡œê·¸ ìƒì„±"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ë“œëŸ¼ íƒ€ê²© ê²€ì¶œ ê²°ê³¼ (ìµœì¢… ê°œì„  ë²„ì „)\n")
        f.write("=" * 80 + "\n")
        f.write(f"BPM: {bpm:.1f}\n")
        f.write(f"ì´ ì´ë²¤íŠ¸: {len(grouped_events)}ê°œ\n")
        f.write(f"hop_length: 256 (ì‹œê°„ í•´ìƒë„ ~11.6ms)\n")
        f.write(f"Mel normalization: {config.use_mel_normalization}\n")
        f.write(f"ì–‘ìí™” ì„¤ì •:\n")
        for drum_type, division in config.grid_division.items():
            f.write(f"  - {drum_type}: {division}ë¶„ìŒí‘œ ê·¸ë¦¬ë“œ\n")
        f.write(f"Threshold:\n")
        for drum_type, threshold in config.thresholds.items():
            f.write(f"  - {drum_type}: {threshold:.2f}\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"{'ì‹œê°„(ì´ˆ)':>10s} {'ì‹œê°„(ë¶„:ì´ˆ)':>12s} {'ë“œëŸ¼':20s}\n")
        f.write("-" * 80 + "\n")

        for time, drums in grouped_events:
            minutes = int(time // 60)
            seconds = time % 60
            time_str = f"{minutes:02d}:{seconds:05.2f}"
            drums_str = " + ".join(drums)
            f.write(f"{time:10.3f} {time_str:>12s} {drums_str:20s}\n")

    print(f"   âœ… ë””ë²„ê·¸ ë¡œê·¸ ì €ì¥: {output_path}")


# ============================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================
def drum_wav_to_midi(
    wav_path: str,
    model_path: str,
    output_dir: str = None,
    config: InferenceConfig = None,
    bpm_override: float = None
):
    """ë“œëŸ¼ WAV â†’ MIDI ë³€í™˜ ë©”ì¸ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ê°œì„  ë²„ì „)"""
    if config is None:
        config = InferenceConfig()

    print("\n" + "=" * 80)
    print("ğŸ¥ ë“œëŸ¼ WAV â†’ MIDI ì•…ë³´ ë³€í™˜ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ê°œì„  ë²„ì „)")
    print("=" * 80)
    print("ğŸ¯ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("  - âœ… Mel normalization ì ìš©")
    print("  - âœ… Checkpointì—ì„œ ìµœì  threshold ìë™ ë¡œë“œ")
    print("  - âœ… ë“œëŸ¼ë³„ ë…ë¦½ ì–‘ìí™” ê·¸ë¦¬ë“œ")
    print("=" * 80)

    if output_dir is None:
        output_dir = os.path.dirname(wav_path)
    os.makedirs(output_dir, exist_ok=True)

    base_name = Path(wav_path).stem
    midi_path = os.path.join(output_dir, f"{base_name}_drums.mid")
    log_path = os.path.join(output_dir, f"{base_name}_drums.txt")

    # Step 1: ëª¨ë¸ ë¡œë“œ + Threshold ë¡œë“œ (ê°œì„ !)
    print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë”©: {model_path}")

    checkpoint = torch.load(
        model_path,
        map_location=config.device,
        weights_only=False
    )

    # Checkpointì—ì„œ ìµœì  threshold ë¡œë“œ (ê°œì„ !)
    #config.load_thresholds_from_checkpoint(checkpoint)

    model = DrumOnsetDetector(
        n_mels=config.n_mels,
        n_classes=config.n_classes,
        cnn_channels=config.cnn_channels,
        gru_hidden=config.gru_hidden,
        gru_layers=config.gru_layers,
        dropout=config.dropout
    ).to(config.device)

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"   Epoch: {checkpoint['epoch']}")
    if 'metrics' in checkpoint:
        print(f"   Val F1: {checkpoint['metrics'].get('f1_avg', 0):.4f}")

    # Step 2: BPM ê°ì§€
    if bpm_override is not None:
        print(f"\nğŸµ BPM ìˆ˜ë™ ì„¤ì • ì‚¬ìš©: {bpm_override:.1f}")
        bpm = float(bpm_override)
    else:
        bpm = detect_bpm(wav_path, config)

    # Step 3: ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (Mel normalization í¬í•¨)
    mel_spec, duration = load_and_preprocess_audio(wav_path, config)

    n_frames = mel_spec.shape[0]
    frame_times = np.arange(n_frames) * config.frame_duration

    # Step 4: ë“œëŸ¼ íƒ€ê²© ì˜ˆì¸¡
    predictions = predict_drum_onsets(mel_spec, model, config)

    # Step 5: Peak Detection
    onsets = detect_peaks(predictions, config, frame_times)

    # Step 6: 4ë‹¨ê³„ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    onsets = merge_nearby_events(onsets, config)
    onsets = enforce_minimum_gap(onsets, config)
    onsets = quantize_to_grid(onsets, bpm, config)
    grouped_events = group_simultaneous_hits(onsets, config)

    # Step 7: MIDI íŒŒì¼ ìƒì„±
    create_midi_file(grouped_events, bpm, config, midi_path)

    # Step 8: ë””ë²„ê·¸ ë¡œê·¸ ìƒì„±
    create_debug_log(grouped_events, bpm, config, log_path)

    # ì™„ë£Œ
    print("\n" + "=" * 80)
    print("âœ… ë³€í™˜ ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“„ ì…ë ¥ WAV: {wav_path}")
    print(f"ğŸ¼ ì¶œë ¥ MIDI: {midi_path}")
    print(f"ğŸ“ ì¶œë ¥ ë¡œê·¸: {log_path}")
    print(f"â±ï¸  ê¸¸ì´: {duration:.2f}ì´ˆ")
    print(f"ğŸµ BPM: {bpm:.1f}")
    print(f"ğŸ¯ ì´ ì´ë²¤íŠ¸: {len(grouped_events)}ê°œ")
    print("=" * 80 + "\n")

    return midi_path, bpm, grouped_events
# ============================================
# ì‹¤í–‰ ì„¤ì •
# ============================================
if __name__ == "__main__":
    # ====================================
    # ğŸ¯ ì—¬ê¸°ì„œ íŒŒë¼ë¯¸í„° ìˆ˜ì •í•˜ì„¸ìš”!
    # ====================================

    # í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    WAV_PATH = r"D:\model_test\drums.wav"
    MODEL_PATH = r"D:\model_test\new_BIGRU\checkpoints_final\best_model.pt"  # ìµœì¢… ëª¨ë¸ ê²½ë¡œ
    OUTPUT_DIR = None

    # BPM ì„¤ì •
    BPM = None  # Noneì´ë©´ ìë™ ê°ì§€, ìˆ«ì ì…ë ¥ ì‹œ ìˆ˜ë™ ì„¤ì •

    # ì„¤ì • ê°ì²´ ìƒì„±
    config = InferenceConfig()

    # ====================================
    # ğŸµ ë“œëŸ¼ë³„ ê·¸ë¦¬ë“œ ì„¤ì • (ì„ íƒì‚¬í•­)
    # ====================================
    config.grid_division['kick'] = 16
    config.grid_division['snare'] = 8
    config.grid_division['hihat'] = 8

    config.quantize_bias['kick'] = 1
    config.quantize_bias['snare'] = 0.3
    config.quantize_bias['hihat'] = 0.25

    # ====================================
    # ğŸ”§ ì„ê³„ê°’ ìˆ˜ë™ ì¡°ì • (ì„ íƒì‚¬í•­)
    # ====================================
    # âš ï¸  ì£¼ì˜: Checkpointì—ì„œ ìë™ ë¡œë“œë˜ë¯€ë¡œ í•„ìš”ì‹œì—ë§Œ ìˆ˜ì •
    config.thresholds['kick'] = 0.5
    config.thresholds['snare'] = 0.5
    config.thresholds['hihat'] = 0.15

    # ====================================
    # ğŸ”§ í›„ì²˜ë¦¬ ì„¤ì • ì¡°ì • (ì„ íƒì‚¬í•­)
    # ====================================
    config.merge_window_ms = 50

    config.min_gap_ms['kick'] = 80
    config.min_gap_ms['snare'] = 60
    config.min_gap_ms['hihat'] = 30

    config.simultaneous_window_ms = 30

    # ====================================
    # ğŸ¼ MIDI ì¶œë ¥ ì„¤ì • (ì„ íƒì‚¬í•­)
    # ====================================
    config.velocity = 100
    config.note_duration = 0.1

    # ====================================
    # ğŸš€ ì‹¤í–‰
    # ====================================
    print("\n" + "=" * 80)
    print("ğŸ›ï¸  í˜„ì¬ ì„¤ì •:")
    print("=" * 80)
    print(f"ğŸ“„ WAV íŒŒì¼: {WAV_PATH}")
    print(f"ğŸ¤– ëª¨ë¸ íŒŒì¼: {MODEL_PATH}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR if OUTPUT_DIR else 'WAV íŒŒì¼ê³¼ ê°™ì€ ìœ„ì¹˜'}")
    print(f"ğŸµ BPM: {BPM if BPM else 'ìë™ ê°ì§€'}")
    print(f"âœ… Mel normalization: {config.use_mel_normalization}")
    print(f"\nğŸµ ë“œëŸ¼ë³„ ì–‘ìí™” ê·¸ë¦¬ë“œ:")
    print(f"  - Kick:  {config.grid_division['kick']}ë¶„ìŒí‘œ")
    print(f"  - Snare: {config.grid_division['snare']}ë¶„ìŒí‘œ")
    print(f"  - Hihat: {config.grid_division['hihat']}ë¶„ìŒí‘œ")
    print("=" * 80)

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    drum_wav_to_midi(
        wav_path=WAV_PATH,
        model_path=MODEL_PATH,
        output_dir=OUTPUT_DIR,
        config=config,
        bpm_override=BPM
    )