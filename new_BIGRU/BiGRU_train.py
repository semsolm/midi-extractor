"""
ë“œëŸ¼ íƒ€ê²© ê²€ì¶œ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ê°œì„  ë²„ì „)

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. âœ… Â±1 í”„ë ˆì„ í™•ì¥ ë¼ë²¨
2. âœ… SpecAugment ì¶”ê°€
3. âœ… Silent sample augmentation
4. âœ… WeightedFocalBCEWithLogitsLoss êµ¬í˜„
5. âœ… í´ë˜ìŠ¤ë³„ threshold grid search
6. âœ… Best thresholdë¥¼ checkpointì— ì €ì¥
"""

import os
import numpy as np
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.metrics import f1_score, precision_score, recall_score
from tqdm import tqdm
import json
from datetime import datetime
import gc
import time
from itertools import product

from BiGRU_model import DrumOnsetDetector
from BiGRU_datautilr import DrumDatasetConfig, get_dataloaders


class CompleteTrainConfig:
    """ì™„ì „í•œ í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤ (ìµœì¢… ê°œì„  ë²„ì „)"""

    def __init__(self):
        # ê²½ë¡œ ì„¤ì •
        self.save_dir = "./checkpoints_final"
        self.log_dir = "./logs_final"

        # ë°ì´í„° ë¡œë”© ëª¨ë“œ ì„ íƒ
        self.use_precomputed = True
        self.precomputed_root = "./precomputed_bigru_data_hop256_improved"

        # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.epochs = 50
        self.batch_size = 4
        self.accumulation_steps = 4  # ì‹¤ì§ˆ ë°°ì¹˜: 16
        self.learning_rate = 1e-3
        self.weight_decay = 1e-4
        self.num_workers = 4

        # ìµœì í™” ì„¤ì •
        self.use_mixed_precision = True
        self.use_gradient_checkpointing = False
        self.empty_cache_every_n_batches = 20
        self.max_grad_norm = 1.0

        # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.n_mels = 128
        self.n_classes = 3
        self.cnn_channels = [32, 64, 128]
        self.gru_hidden = 384
        self.gru_layers = 2
        self.dropout = 0.3

        # ============================================
        # ì†ì‹¤í•¨ìˆ˜ ì„ íƒ ë° ê°€ì¤‘ì¹˜
        # ============================================
        self.use_focal_loss = True  # True: Focal Loss, False: ì¼ë°˜ BCE
        self.focal_alpha = 0.25  # Focal Loss alpha
        self.focal_gamma = 2.0   # Focal Loss gamma

        # í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ (kick/snareê°€ ì ê²Œ ë‚˜ì˜¤ë¯€ë¡œ)
        self.class_weights = [2.0, 1.5, 1.0]  # [kick, snare, hihat]

        # ============================================
        # í´ë˜ìŠ¤ë³„ Threshold ì„¤ì •
        # ============================================
        # ì´ˆê¸°ê°’ (í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ìµœì í™”ë¨)
        self.thresholds = [0.5, 0.5, 0.5]  # [kick, snare, hihat]

        # Threshold íƒìƒ‰ ì„¤ì •
        self.search_thresholds = True  # validation ì‹œ threshold íƒìƒ‰
        self.threshold_search_grid = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]

        # Early stopping ì„¤ì •
        self.patience = 10

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True

        self._print_config()

        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)

    def _print_config(self):
        """ì„¤ì • ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸš€ í•™ìŠµ ì„¤ì • (ìµœì¢… ê°œì„  ë²„ì „)")
        print("=" * 80)
        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {self.save_dir}")
        print(f"ğŸ“Š ë¡œê·¸ ê²½ë¡œ: {self.log_dir}")
        print(f"\nâš¡ ë°ì´í„° ë¡œë”©: {'ê³ ì† ëª¨ë“œ (ì‚¬ì „ ê³„ì‚°)' if self.use_precomputed else 'ì¼ë°˜ ëª¨ë“œ'}")
        print(f"ğŸ”¢ ì—í¬í¬: {self.epochs}")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"ğŸ”„ Accumulation steps: {self.accumulation_steps} (ì‹¤ì§ˆ ë°°ì¹˜: {self.batch_size * self.accumulation_steps})")
        print(f"ğŸ¯ í•™ìŠµë¥ : {self.learning_rate}")
        print(f"ğŸ‘· ì›Œì»¤ ìˆ˜: {self.num_workers}")
        print(f"ğŸ¨ Mixed precision: {self.use_mixed_precision}")

        loss_type = "Focal BCE Loss" if self.use_focal_loss else "Weighted BCE Loss"
        print(f"\nğŸ“ ì†ì‹¤í•¨ìˆ˜: {loss_type}")
        if self.use_focal_loss:
            print(f"   Focal alpha: {self.focal_alpha}, gamma: {self.focal_gamma}")
        print(f"ğŸ›ï¸  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: kick={self.class_weights[0]}, snare={self.class_weights[1]}, hihat={self.class_weights[2]}")

        print(f"\nğŸ¯ Threshold ì´ˆê¸°ê°’: kick={self.thresholds[0]}, snare={self.thresholds[1]}, hihat={self.thresholds[2]}")
        print(f"ğŸ” Threshold ìë™ íƒìƒ‰: {self.search_thresholds}")

        print(f"\nğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {self.device}")
        if self.device.type == 'cuda':
            print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        print("=" * 80 + "\n")


def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


class WeightedFocalBCEWithLogitsLoss(nn.Module):
    """
    Focal Loss + í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ Binary Cross Entropy Loss

    Focal LossëŠ” ì‰¬ìš´ ìƒ˜í”Œì˜ ê¸°ì—¬ë„ë¥¼ ì¤„ì´ê³  ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ë„ë¡ í•¨
    - alpha: positive/negative ìƒ˜í”Œì˜ ê· í˜• ì¡°ì ˆ
    - gamma: ì‰¬ìš´ ìƒ˜í”Œì˜ loss ê°ì†Œ ì •ë„ (gamma=0ì´ë©´ ì¼ë°˜ BCE)

    Formula: FL(p_t) = -alpha_t * (1-p_t)^gamma * log(p_t)
    """

    def __init__(self, pos_weights=None, alpha=0.25, gamma=2.0):
        super().__init__()
        self.pos_weights = pos_weights
        self.alpha = alpha
        self.gamma = gamma

        if pos_weights is not None:
            self.pos_weights = torch.FloatTensor(pos_weights)

    def forward(self, logits, targets, lengths=None):
        """
        Args:
            logits: (B, T, C) - ëª¨ë¸ ì¶œë ¥
            targets: (B, T, C) - ì •ë‹µ ë ˆì´ë¸”
            lengths: (B,) - ê° ì‹œí€€ìŠ¤ì˜ ì‹¤ì œ ê¸¸ì´ (optional)
        """
        if self.pos_weights is not None:
            self.pos_weights = self.pos_weights.to(logits.device)

        # BCE loss (reduction='none')
        bce_loss = nn.functional.binary_cross_entropy_with_logits(
            logits, targets,
            pos_weight=self.pos_weights,
            reduction='none'  # (B, T, C)
        )

        # Focal loss ê³„ì‚°
        probs = torch.sigmoid(logits)
        p_t = targets * probs + (1 - targets) * (1 - probs)  # p_t
        focal_weight = (1 - p_t) ** self.gamma  # (1-p_t)^gamma

        # Alpha balancing
        alpha_t = targets * self.alpha + (1 - targets) * (1 - self.alpha)

        # Final focal loss
        focal_loss = alpha_t * focal_weight * bce_loss

        # Masking ì ìš© (padding ì˜ì—­ ì œì™¸)
        if lengths is not None:
            B, T, C = focal_loss.shape
            mask = torch.arange(T, device=logits.device).unsqueeze(0) < lengths.unsqueeze(1)
            mask = mask.unsqueeze(2).expand(B, T, C)

            focal_loss = focal_loss * mask.float()

            # ìœ íš¨í•œ í”„ë ˆì„ ìˆ˜ë¡œ í‰ê· 
            valid_count = mask.sum()
            if valid_count > 0:
                focal_loss = focal_loss.sum() / valid_count
            else:
                focal_loss = focal_loss.mean()
        else:
            focal_loss = focal_loss.mean()

        return focal_loss


class WeightedBCEWithLogitsLoss(nn.Module):
    """í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ Binary Cross Entropy Loss (Masking í¬í•¨)"""

    def __init__(self, pos_weights=None):
        super().__init__()
        self.pos_weights = pos_weights
        if pos_weights is not None:
            self.pos_weights = torch.FloatTensor(pos_weights)

    def forward(self, logits, targets, lengths=None):
        """
        Args:
            logits: (B, T, C)
            targets: (B, T, C)
            lengths: (B,) - ê° ì‹œí€€ìŠ¤ì˜ ì‹¤ì œ ê¸¸ì´ (optional)
        """
        if self.pos_weights is not None:
            self.pos_weights = self.pos_weights.to(logits.device)

        # ê¸°ë³¸ BCE loss
        loss = nn.functional.binary_cross_entropy_with_logits(
            logits, targets,
            pos_weight=self.pos_weights,
            reduction='none'  # (B, T, C)
        )

        # Masking ì ìš© (padding ì˜ì—­ ì œì™¸)
        if lengths is not None:
            B, T, C = loss.shape
            mask = torch.arange(T, device=logits.device).unsqueeze(0) < lengths.unsqueeze(1)
            mask = mask.unsqueeze(2).expand(B, T, C)

            loss = loss * mask.float()

            # ìœ íš¨í•œ í”„ë ˆì„ ìˆ˜ë¡œ í‰ê· 
            valid_count = mask.sum()
            if valid_count > 0:
                loss = loss.sum() / valid_count
            else:
                loss = loss.mean()
        else:
            loss = loss.mean()

        return loss


def search_best_thresholds(all_probs, all_labels, all_lengths, config):
    """
    Grid searchë¡œ í´ë˜ìŠ¤ë³„ ìµœì  threshold íƒìƒ‰

    Args:
        all_probs: List of (B, T, C) probability tensors
        all_labels: List of (B, T, C) label tensors
        all_lengths: List of (B,) length tensors
        config: ì„¤ì •

    Returns:
        best_thresholds: [kick_th, snare_th, hihat_th]
        best_f1: ìµœê³  F1 score
    """
    # ë°ì´í„° flatí•˜ê²Œ ëª¨ìœ¼ê¸°
    flat_probs = []
    flat_labels = []

    for probs_batch, labels_batch, lengths_batch in zip(all_probs, all_labels, all_lengths):
        B, T, C = probs_batch.shape
        lengths_batch = lengths_batch.cpu()

        for i in range(B):
            L = int(lengths_batch[i].item())
            probs_valid = probs_batch[i, :L, :].cpu().numpy()
            labels_valid = labels_batch[i, :L, :].cpu().numpy()

            flat_probs.append(probs_valid)
            flat_labels.append(labels_valid)

    if len(flat_probs) == 0:
        return config.thresholds, 0.0

    all_probs_np = np.vstack(flat_probs)  # (N, C)
    all_labels_np = np.vstack(flat_labels)  # (N, C)

    # Grid search
    best_f1 = 0.0
    best_thresholds = config.thresholds.copy()

    grid = config.threshold_search_grid

    print(f"\nğŸ” Threshold íƒìƒ‰ ì¤‘... (grid: {grid})")

    # ëª¨ë“  ì¡°í•© íƒìƒ‰
    for th_kick, th_snare, th_hihat in product(grid, grid, grid):
        thresholds = [th_kick, th_snare, th_hihat]

        # ì˜ˆì¸¡
        preds = np.zeros_like(all_probs_np, dtype=int)
        for class_idx in range(config.n_classes):
            preds[:, class_idx] = (all_probs_np[:, class_idx] >= thresholds[class_idx]).astype(int)

        # F1 ê³„ì‚°
        f1_scores = []
        for class_idx in range(config.n_classes):
            f1 = f1_score(
                all_labels_np[:, class_idx],
                preds[:, class_idx],
                zero_division=0
            )
            f1_scores.append(f1)

        avg_f1 = np.mean(f1_scores)

        if avg_f1 > best_f1:
            best_f1 = avg_f1
            best_thresholds = thresholds

    print(f"âœ… ìµœì  Threshold ë°œê²¬:")
    print(f"   kick={best_thresholds[0]}, snare={best_thresholds[1]}, hihat={best_thresholds[2]}")
    print(f"   F1={best_f1:.4f}")

    return best_thresholds, best_f1


def compute_metrics_from_lists(all_preds, all_labels, all_lengths, thresholds):
    """
    ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘ëœ ì˜ˆì¸¡/ë ˆì´ë¸”/ê¸¸ì´ì—ì„œ ë©”íŠ¸ë¦­ ê³„ì‚°
    í´ë˜ìŠ¤ë³„ threshold ì ìš©

    Args:
        all_preds: List of (B, T, C) probability tensors (sigmoid í†µê³¼)
        all_labels: List of (B, T, C) label tensors
        all_lengths: List of (B,) length tensors
        thresholds: [kick_th, snare_th, hihat_th]
    """
    flat_preds = []
    flat_labels = []

    for preds_batch, labels_batch, lengths_batch in zip(all_preds, all_labels, all_lengths):
        probs_batch = preds_batch
        B, T, C = probs_batch.shape

        lengths_batch = lengths_batch.cpu()

        for i in range(B):
            L = int(lengths_batch[i].item())
            probs_valid = probs_batch[i, :L, :].cpu().numpy()
            labels_valid = labels_batch[i, :L, :].cpu().numpy()

            flat_preds.append(probs_valid)
            flat_labels.append(labels_valid)

    if len(flat_preds) == 0:
        return {
            'f1_kick': 0.0,
            'f1_snare': 0.0,
            'f1_hihat': 0.0,
            'f1_avg': 0.0,
            'precision_avg': 0.0,
            'recall_avg': 0.0
        }

    all_preds_np = np.vstack(flat_preds)  # (N, C)
    all_labels_np = np.vstack(flat_labels)  # (N, C)

    # í´ë˜ìŠ¤ë³„ threshold ì ìš©
    all_preds_binary = np.zeros_like(all_preds_np, dtype=int)
    for class_idx in range(len(thresholds)):
        threshold = thresholds[class_idx]
        all_preds_binary[:, class_idx] = (all_preds_np[:, class_idx] >= threshold).astype(int)

    f1_per_class, precision_per_class, recall_per_class = [], [], []

    for class_idx in range(len(thresholds)):
        f1 = f1_score(
            all_labels_np[:, class_idx],
            all_preds_binary[:, class_idx],
            zero_division=0
        )
        precision = precision_score(
            all_labels_np[:, class_idx],
            all_preds_binary[:, class_idx],
            zero_division=0
        )
        recall = recall_score(
            all_labels_np[:, class_idx],
            all_preds_binary[:, class_idx],
            zero_division=0
        )

        f1_per_class.append(f1)
        precision_per_class.append(precision)
        recall_per_class.append(recall)

    metrics = {
        'f1_kick': f1_per_class[0],
        'f1_snare': f1_per_class[1],
        'f1_hihat': f1_per_class[2],
        'f1_avg': np.mean(f1_per_class),
        'precision_avg': np.mean(precision_per_class),
        'recall_avg': np.mean(recall_per_class),
        'precision_kick': precision_per_class[0],
        'precision_snare': precision_per_class[1],
        'precision_hihat': precision_per_class[2],
        'recall_kick': recall_per_class[0],
        'recall_snare': recall_per_class[1],
        'recall_hihat': recall_per_class[2]
    }

    return metrics


def train_one_epoch(model, train_loader, criterion, optimizer, scaler, config):
    """1 ì—í¬í¬ í•™ìŠµ (Masking ì ìš©)"""
    model.train()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    all_lengths = []

    optimizer.zero_grad()

    pbar = tqdm(train_loader, desc="Training", ncols=100)
    for batch_idx, (mel_specs, labels, lengths) in enumerate(pbar):
        mel_specs = mel_specs.to(config.device, non_blocking=True)
        labels = labels.to(config.device, non_blocking=True)
        lengths = lengths.to(config.device, non_blocking=True)

        with torch.amp.autocast('cuda', enabled=config.use_mixed_precision):
            logits = model(mel_specs)
            loss = criterion(logits, labels, lengths)
            loss = loss / config.accumulation_steps

        scaler.scale(loss).backward()

        # ì˜ˆì¸¡ ì €ì¥
        with torch.no_grad():
            probs = torch.sigmoid(logits)
            all_preds.append(probs.cpu())
            all_labels.append(labels.cpu())
            all_lengths.append(lengths.cpu())

        total_loss += loss.item() * config.accumulation_steps

        pbar.set_postfix({'loss': f"{loss.item() * config.accumulation_steps:.4f}"})

        # Gradient accumulation
        if (batch_idx + 1) % config.accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if (batch_idx + 1) % config.empty_cache_every_n_batches == 0:
            clear_gpu_memory()

    # ë‚¨ì€ gradient ì²˜ë¦¬
    if len(train_loader) % config.accumulation_steps != 0:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()

    avg_loss = total_loss / len(train_loader)

    metrics = compute_metrics_from_lists(all_preds, all_labels, all_lengths, config.thresholds)
    metrics['loss'] = avg_loss
    return metrics


def validate(model, val_loader, criterion, config):
    """
    ê²€ì¦ (Masking ì ìš© + Threshold íƒìƒ‰)

    Returns:
        metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        best_thresholds: íƒìƒ‰ëœ ìµœì  threshold (search_thresholds=Trueì¼ ë•Œ)
    """
    model.eval()
    total_loss = 0.0
    all_logits = []
    all_labels = []
    all_lengths = []

    with torch.no_grad():
        pbar = tqdm(val_loader, desc="Validation", ncols=100)
        for batch_idx, (mel_specs, labels, lengths) in enumerate(pbar):
            mel_specs = mel_specs.to(config.device, non_blocking=True)
            labels = labels.to(config.device, non_blocking=True)
            lengths = lengths.to(config.device, non_blocking=True)

            with torch.amp.autocast('cuda', enabled=config.use_mixed_precision):
                logits = model(mel_specs)
                loss = criterion(logits, labels, lengths)

            total_loss += loss.item()

            # Logits ì €ì¥ (threshold íƒìƒ‰ìš©)
            all_logits.append(logits.cpu())
            all_labels.append(labels.cpu())
            all_lengths.append(lengths.cpu())

            pbar.set_postfix({'loss': f"{loss.item():.4f}"})

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if (batch_idx + 1) % config.empty_cache_every_n_batches == 0:
                clear_gpu_memory()

    avg_loss = total_loss / len(val_loader)

    # Sigmoid ì ìš©
    all_probs = [torch.sigmoid(logits) for logits in all_logits]

    # Threshold íƒìƒ‰
    best_thresholds = config.thresholds
    if config.search_thresholds:
        best_thresholds, _ = search_best_thresholds(
            all_probs, all_labels, all_lengths, config
        )

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = compute_metrics_from_lists(all_probs, all_labels, all_lengths, best_thresholds)
    metrics['loss'] = avg_loss

    return metrics, best_thresholds
"""
ë“œëŸ¼ íƒ€ê²© ê²€ì¶œ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ - Part 2
Checkpoint ì €ì¥ ë° ë©”ì¸ í•™ìŠµ ë£¨í”„
"""

import os
import torch
import json


def save_checkpoint(model, optimizer, scheduler, epoch, metrics, config, filename, thresholds=None):
    """
    ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ìµœì  threshold í¬í•¨)

    Args:
        model: ëª¨ë¸
        optimizer: Optimizer
        scheduler: Scheduler
        epoch: í˜„ì¬ ì—í¬í¬
        metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        config: ì„¤ì •
        filename: ì €ì¥ íŒŒì¼ëª…
        thresholds: ìµœì  threshold [kick, snare, hihat] (optional)
    """
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'metrics': metrics,
        'config': {
            'n_mels': config.n_mels,
            'n_classes': config.n_classes,
            'cnn_channels': config.cnn_channels,
            'gru_hidden': config.gru_hidden,
            'gru_layers': config.gru_layers,
            'dropout': config.dropout,
            'thresholds': thresholds if thresholds is not None else config.thresholds,
            'class_weights': config.class_weights,
            'use_focal_loss': config.use_focal_loss,
            'focal_alpha': config.focal_alpha if config.use_focal_loss else None,
            'focal_gamma': config.focal_gamma if config.use_focal_loss else None,
        }
    }

    save_path = os.path.join(config.save_dir, filename)
    torch.save(checkpoint, save_path)
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {save_path}")
    if thresholds is not None:
        print(f"   ìµœì  Threshold: kick={thresholds[0]:.2f}, snare={thresholds[1]:.2f}, hihat={thresholds[2]:.2f}")


def train(config):
    """ì „ì²´ í•™ìŠµ ë£¨í”„"""
    print("\n" + "=" * 80)
    print("ğŸš€ í•™ìŠµ ì‹œì‘ (ìµœì¢… ê°œì„  ë²„ì „)")
    print("=" * 80)

    # ë°ì´í„°ì…‹ ì„¤ì •
    data_config = DrumDatasetConfig()
    data_config.use_precomputed = config.use_precomputed
    data_config.precomputed_root = config.precomputed_root

    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader, val_loader, test_loader = get_dataloaders(
        data_config,
        batch_size=config.batch_size,
        num_workers=config.num_workers
    )

    # ëª¨ë¸ ìƒì„±
    print("\nğŸ“¦ ëª¨ë¸ ì´ˆê¸°í™” (ìµœì¢… ê°œì„  ë²„ì „)...")
    model = DrumOnsetDetector(
        n_mels=config.n_mels,
        n_classes=config.n_classes,
        cnn_channels=config.cnn_channels,
        gru_hidden=config.gru_hidden,
        gru_layers=config.gru_layers,
        dropout=config.dropout
    ).to(config.device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"   í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}")

    # ì†ì‹¤í•¨ìˆ˜ ì„ íƒ
    if config.use_focal_loss:
        print(f"\nğŸ“ ì†ì‹¤í•¨ìˆ˜: Focal Loss (alpha={config.focal_alpha}, gamma={config.focal_gamma})")
        criterion = WeightedFocalBCEWithLogitsLoss(
            pos_weights=config.class_weights,
            alpha=config.focal_alpha,
            gamma=config.focal_gamma
        )
    else:
        print(f"\nğŸ“ ì†ì‹¤í•¨ìˆ˜: Weighted BCE Loss")
        criterion = WeightedBCEWithLogitsLoss(pos_weights=config.class_weights)

    # Optimizer
    optimizer = AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    # Scheduler
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='max',
        factor=0.5,
        patience=5,
    )

    # Mixed precision scaler
    scaler = torch.amp.GradScaler('cuda', enabled=config.use_mixed_precision)

    # í•™ìŠµ ê¸°ë¡
    history = {
        'train_loss': [], 'train_f1': [],
        'val_loss': [], 'val_f1': [],
        'thresholds_history': []  # Threshold ë³€í™” ì¶”ì 
    }

    best_f1 = 0.0
    best_thresholds = config.thresholds.copy()
    patience_counter = 0

    print("\n" + "=" * 80)
    print("ğŸ¯ í•™ìŠµ ì‹œì‘!")
    print("=" * 80)

    for epoch in range(1, config.epochs + 1):
        print(f"\nğŸ“ Epoch [{epoch}/{config.epochs}]")
        print("-" * 80)

        # í•™ìŠµ
        train_metrics = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler, config
        )

        # ê²€ì¦ (Threshold íƒìƒ‰ í¬í•¨)
        val_metrics, searched_thresholds = validate(model, val_loader, criterion, config)

        # Threshold ì—…ë°ì´íŠ¸
        if config.search_thresholds:
            config.thresholds = searched_thresholds

        # Scheduler step
        scheduler.step(val_metrics['f1_avg'])

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š [TRAIN] Loss: {train_metrics['loss']:.4f} | "
              f"F1_avg: {train_metrics['f1_avg']:.4f} | "
              f"F1_kick: {train_metrics['f1_kick']:.4f} | "
              f"F1_snare: {train_metrics['f1_snare']:.4f} | "
              f"F1_hihat: {train_metrics['f1_hihat']:.4f}")

        print(f"ğŸ“Š [VAL]   Loss: {val_metrics['loss']:.4f} | "
              f"F1_avg: {val_metrics['f1_avg']:.4f} | "
              f"F1_kick: {val_metrics['f1_kick']:.4f} | "
              f"F1_snare: {val_metrics['f1_snare']:.4f} | "
              f"F1_hihat: {val_metrics['f1_hihat']:.4f}")

        print(f"   P_kick: {val_metrics['precision_kick']:.3f} | "
              f"R_kick: {val_metrics['recall_kick']:.3f} | "
              f"P_snare: {val_metrics['precision_snare']:.3f} | "
              f"R_snare: {val_metrics['recall_snare']:.3f} | "
              f"P_hihat: {val_metrics['precision_hihat']:.3f} | "
              f"R_hihat: {val_metrics['recall_hihat']:.3f}")

        # ê¸°ë¡ ì €ì¥
        history['train_loss'].append(train_metrics['loss'])
        history['train_f1'].append(train_metrics['f1_avg'])
        history['val_loss'].append(val_metrics['loss'])
        history['val_f1'].append(val_metrics['f1_avg'])
        history['thresholds_history'].append(config.thresholds.copy())

        # Best model ì €ì¥
        if val_metrics['f1_avg'] > best_f1:
            best_f1 = val_metrics['f1_avg']
            best_thresholds = config.thresholds.copy()
            save_checkpoint(
                model, optimizer, scheduler, epoch, val_metrics,
                config, 'best_model.pt', thresholds=best_thresholds
            )
            print(f"ğŸ‰ ìƒˆë¡œìš´ Best F1: {best_f1:.4f}")
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"â³ Patience: {patience_counter}/{config.patience}")

        # ì£¼ê¸°ì  ì €ì¥
        if epoch % 10 == 0:
            save_checkpoint(
                model, optimizer, scheduler, epoch, val_metrics,
                config, f'checkpoint_epoch_{epoch}.pt', thresholds=config.thresholds
            )

        # Early stopping
        if patience_counter >= config.patience:
            print(f"\nâ›” Early stopping at epoch {epoch}")
            break

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()

    # ìµœì¢… í‰ê°€
    print("\n" + "=" * 80)
    print("ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€")
    print("=" * 80)

    best_model_path = os.path.join(config.save_dir, 'best_model.pt')
    checkpoint = torch.load(
        best_model_path,
        map_location=config.device,
        weights_only=False
    )
    model.load_state_dict(checkpoint['model_state_dict'])

    # Best threshold ë¡œë“œ
    loaded_thresholds = checkpoint['config']['thresholds']
    print(f"\nâœ… Best threshold ë¡œë“œ: kick={loaded_thresholds[0]:.2f}, snare={loaded_thresholds[1]:.2f}, hihat={loaded_thresholds[2]:.2f}")

    # í…ŒìŠ¤íŠ¸ í‰ê°€ (threshold íƒìƒ‰ ì•ˆí•¨)
    config.search_thresholds = False
    config.thresholds = loaded_thresholds

    test_metrics, _ = validate(model, test_loader, criterion, config)

    print(f"\nğŸ“Š [TEST] Loss: {test_metrics['loss']:.4f} | "
          f"F1_avg: {test_metrics['f1_avg']:.4f} | "
          f"F1_kick: {test_metrics['f1_kick']:.4f} | "
          f"F1_snare: {test_metrics['f1_snare']:.4f} | "
          f"F1_hihat: {test_metrics['f1_hihat']:.4f}")

    print(f"   P_kick: {test_metrics['precision_kick']:.3f} | "
          f"R_kick: {test_metrics['recall_kick']:.3f} | "
          f"P_snare: {test_metrics['precision_snare']:.3f} | "
          f"R_snare: {test_metrics['recall_snare']:.3f} | "
          f"P_hihat: {test_metrics['precision_hihat']:.3f} | "
          f"R_hihat: {test_metrics['recall_hihat']:.3f}")

    # í•™ìŠµ ê¸°ë¡ ì €ì¥
    history['test_metrics'] = test_metrics
    history['best_thresholds'] = best_thresholds

    history_path = os.path.join(config.log_dir, 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)

    print(f"\nğŸ’¾ í•™ìŠµ ê¸°ë¡ ì €ì¥: {history_path}")

    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸŠ í•™ìŠµ ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"Best Validation F1: {best_f1:.4f}")
    print(f"Test F1: {test_metrics['f1_avg']:.4f}")
    print(f"Best Thresholds: kick={best_thresholds[0]:.2f}, snare={best_thresholds[1]:.2f}, hihat={best_thresholds[2]:.2f}")
    print("=" * 80)


if __name__ == "__main__":
    config = CompleteTrainConfig()
    clear_gpu_memory()
    train(config)