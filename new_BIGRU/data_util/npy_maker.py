"""
BiGRUìš© ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì‚¬ì „ ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ê°œì„  ë²„ì „)
hop_length=256, Â±1 í”„ë ˆì„ í™•ì¥ ë¼ë²¨ë¡œ WAV â†’ Mel ë³€í™˜

ì£¼ìš” ê°œì„ ì‚¬í•­:
- Â±1 í”„ë ˆì„ í™•ì¥ ë¼ë²¨ ì ìš©
- í•™ìŠµ ì†ë„ë¥¼ 5~20ë°° í–¥ìƒ
"""

import os
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import multiprocessing as mp

from BiGRU_datautilr import DrumDatasetConfig
from precompute_worker import process_single_file

class PrecomputeConfig:
    """ì‚¬ì „ ê³„ì‚° ì„¤ì • (ìµœì¢… ê°œì„  ë²„ì „)"""
    def __init__(self):
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ (Â±1 í”„ë ˆì„ í™•ì¥ ë¼ë²¨ìš©)
        self.output_root = "./precomputed_bigru_data_hop256_improved"

        # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
        self.num_processes = 8  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •

        # ë°ì´í„°ì…‹ ì„¤ì •
        self.dataset_config = DrumDatasetConfig()


def load_and_filter_metadata(config: DrumDatasetConfig) -> pd.DataFrame:
    """ë©”íƒ€ë°ì´í„° ë¡œë”© ë° í•„í„°ë§"""
    df = pd.read_csv(config.csv_path)
    df = df[~df['style'].str.contains('jazz', case=False, na=False)]
    df = df[df['duration'] > config.min_duration]
    return df


def precompute_dataset(config: PrecomputeConfig):
    """ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ì „ ê³„ì‚°"""
    print("=" * 80)
    print("ğŸš€ BiGRUìš© ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì‚¬ì „ ê³„ì‚° ì‹œì‘ (ìµœì¢… ê°œì„  ë²„ì „)")
    print("=" * 80)
    print(f"ğŸ“ hop_length: {config.dataset_config.hop_length}")
    print(f"ğŸ“ í”„ë ˆì„ ê°„ê²©: {config.dataset_config.frame_duration*1000:.1f}ms")
    print(f"ğŸ“ ë¼ë²¨ í™•ì¥: Â±{config.dataset_config.label_spread_frames} í”„ë ˆì„")
    print("=" * 80)

    dataset_config = config.dataset_config

    # ë©”íƒ€ë°ì´í„° ë¡œë”©
    print("\nğŸ“‚ ë©”íƒ€ë°ì´í„° ë¡œë”© ì¤‘...")
    df = load_and_filter_metadata(dataset_config)

    print(f"âœ… ì´ {len(df)} íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •")

    # Splitë³„ ì²˜ë¦¬
    for split in ['train', 'validation', 'test']:
        split_df = df[df['split'] == split].reset_index(drop=True)

        print(f"\n{'=' * 80}")
        print(f"ğŸ“Š [{split.upper()}] ì²˜ë¦¬ ì¤‘: {len(split_df)} íŒŒì¼")
        print(f"{'=' * 80}")

        # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ìœ„í•œ ì¸ì ì¤€ë¹„
        args_list = [
            (row, config.output_root, split)
            for _, row in split_df.iterrows()
        ]

        print(f"ğŸ”§ {config.num_processes}ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...")

        ctx = mp.get_context("spawn")  # Windows ì•ˆì „í•˜ê²Œ
        with ctx.Pool(processes=config.num_processes) as pool:
            results = list(tqdm(
                pool.imap(process_single_file, args_list),
                total=len(args_list),
                desc=f"{split}"
            ))

        success_count = sum(results)
        print(f"âœ… {split}: {success_count}/{len(split_df)} íŒŒì¼ ì„±ê³µ")

    print("\n" + "=" * 80)
    print("âœ… ì‚¬ì „ ê³„ì‚° ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {config.output_root}")


def verify_precomputed_data(config: PrecomputeConfig):
    """ì‚¬ì „ ê³„ì‚°ëœ ë°ì´í„° ê²€ì¦"""
    print("\n" + "=" * 80)
    print("ğŸ” ë°ì´í„° ê²€ì¦ ì¤‘...")
    print("=" * 80)

    for split in ['train', 'validation', 'test']:
        mel_dir = os.path.join(config.output_root, split, 'mel')
        label_dir = os.path.join(config.output_root, split, 'label')

        if not os.path.exists(mel_dir) or not os.path.exists(label_dir):
            print(f"âŒ {split}: ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        mel_files = list(Path(mel_dir).glob('*.npy'))
        label_files = list(Path(label_dir).glob('*.npy'))

        print(f"\n{split.upper()}:")
        print(f"  Mel files: {len(mel_files)}")
        print(f"  Label files: {len(label_files)}")

        if mel_files:
            sample_mel = np.load(mel_files[0])
            sample_label = np.load(str(mel_files[0]).replace('/mel/', '/label/').replace('\\mel\\', '\\label\\'))

            print(f"  ìƒ˜í”Œ shape:")
            print(f"    Mel: {sample_mel.shape}")
            print(f"    Label: {sample_label.shape}")
            print(f"    í”„ë ˆì„ ìˆ˜: {sample_mel.shape[0]}")

            # ë¼ë²¨ ë°€ë„ í™•ì¸ (Â±1 í”„ë ˆì„ í™•ì¥ìœ¼ë¡œ ì¦ê°€í•´ì•¼ í•¨)
            label_density = sample_label.sum() / (sample_label.shape[0] * sample_label.shape[1])
            print(f"    Label density: {label_density:.4f} (Â±1 í”„ë ˆì„ í™•ì¥ìœ¼ë¡œ ì¦ê°€)")

    print("\nâœ… ê²€ì¦ ì™„ë£Œ!")


if __name__ == "__main__":
    config = PrecomputeConfig()

    print("=" * 80)
    print("âš™ï¸  ì‚¬ì „ ê³„ì‚° ì„¤ì • (ìµœì¢… ê°œì„  ë²„ì „)")
    print("=" * 80)
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_root}")
    print(f"í”„ë¡œì„¸ìŠ¤ ìˆ˜: {config.num_processes}")
    print(f"ë°ì´í„° ë£¨íŠ¸: {config.dataset_config.data_root}")
    print(f"hop_length: {config.dataset_config.hop_length}")
    print(f"í”„ë ˆì„ ê°„ê²©: {config.dataset_config.frame_duration*1000:.1f}ms")
    print(f"ë¼ë²¨ í™•ì¥: Â±{config.dataset_config.label_spread_frames} í”„ë ˆì„")
    print("=" * 80)

    response = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if response.lower() != 'y':
        print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        exit()

    precompute_dataset(config)
    verify_precomputed_data(config)

    print("\n" + "=" * 80)
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("=" * 80)
    print("1. BiGRU_datautilr_improved.pyë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì„¸ìš”.")
    print("2. BiGRU_train_improved.pyì—ì„œ use_precomputed=Trueë¡œ ì„¤ì •")
    print("3. í•™ìŠµ ì†ë„ê°€ 5~20ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤!")
    print("=" * 80)